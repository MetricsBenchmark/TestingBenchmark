import sys
import load_data as andro
import metrics
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from keras import backend as K
import time
from keras.models import Model
from core.ensemble.model_hp import train_hparam
from DBCV.DBCV import DBCV
import cuml
from cuml import DBSCAN
from torchvision import models
import sklearn
import cluster

model_names = ['basic_dnn', 'deepdrebin']
out_csv = 'report/andro.csv'
test_dir = 'test/andro'
budgets = [50, 100, 150, 200]
trainX, valX, testX, trainy, valy, testy = andro.pkl_data_preprocessing()

def get_model(name):
    if name == 'deepdrebin':
        saving_dir = './androzoo'
        dnn_path = os.path.join(saving_dir, 'vanilla', 'dnn')
        model = tf.keras.models.load_model(dnn_path)
    elif name == 'basic_dnn':
        saving_dir = './androzoo'
        dnn_path = os.path.join(saving_dir, 'vanilla', 'basic_dnn')
        model = tf.keras.models.load_model(dnn_path)
    return model

def run_selection(model_name, test_set, testX, testy, metricList, budgets, force_save=False):
    model = get_model(model_name)
    for m in metricList:
        for b in budgets:
            test_out_dir = os.path.join(test_dir, test_set, model_name, m, str(b))
            if m == 'dat':
                n_test = testX.shape[0]
                hybridX = np.concatenate((trainX[:int(n_test // 2)], testX[:int(n_test // 2)]), axis=0)
                hybridy = np.concatenate((trainy[:int(n_test // 2)], testy[:int(n_test // 2)]), axis=0)
                selectedX, selectedy, idx = metrics.dat_ood_detector(
                    testX, testy, model, b, test_set, trainX, trainy, hybridX, hybridy, batch_size=128,
                    num_classes=2
                )
            else:
                selectedX, selectedy, idx = metrics.select(trainX, trainy,
                                                           testX, testy, model, b, m, test_set, model_name
                                                           )

            score = model.evaluate(selectedX, selectedy, verbose=0)
            np.savetxt(os.path.join(test_out_dir, 'X.txt'), idx, fmt='%d')
            np.savetxt(os.path.join(test_out_dir, 'y.txt'), selectedy.astype(int), fmt='%d')



def select():
    metricList = ['dr', 'ces', 'mcp', 'est', 'rnd', 'ent', 'gini', 'dat', 'gd', 'kmnc', 'nac', 'lsa', 'dsa', 'std', 'pace']
    for m in model_names:
        run_selection(m, 'andro', testX, testy, metricList, budgets, resave)

    _X, _y = andro.get_ood('2018')
    run_selection('deepdrebin', 'andro_2018', _X, _y, metricList, budgets, resave)

    _X, _y = andro.get_ood('2019')
    run_selection('deepdrebin', 'andro_2019', _X, _y, metricList, budgets, resave)

    _X, _y = andro.get_ood('drebin')
    run_selection('deepdrebin', 'andro_drebin', _X, _y, metricList, budgets, resave)

    _X, _y = andro.get_ood('adv')
    run_selection('deepdrebin', 'andro_adv', _X, _y, metricList, budgets, resave)

    _X, _y = andro.get_ood('label')
    run_selection('deepdrebin', 'andro_label', _X, _y, metricList, budgets, resave)


def rmse(acc, acc_hat, randomness=True):
    if randomness:
        N = len(acc)
        return np.sqrt(1 / N * np.sum((acc_hat - acc) ** 2, axis=1))
    else:
        return np.abs(acc_hat - acc)

def run_evaluation(model_name, test_set, metricList, budgets, fullX, fully, originalX, originaly):
    model = get_model(model_name)
    full_y_int = fully
    full_pred = model.predict(fullX, verbose=0)
    p1 = full_pred
    p0 = 1 - p1
    softmax = np.hstack([p0, p1])
    full_pred_int = np.argmax(softmax, axis=1)

    results = []
    for m in metricList:
        for b in budgets:
            test_out_dir = os.path.join(test_dir, test_set, model_name, m, str(b))
            X_id = np.loadtxt(os.path.join(test_out_dir, 'X.txt'), dtype=int)
            sort = np.zeros(fullX.shape[0], dtype=int)
            sort[X_id] = np.arange(1, b + 1)

            acc_hat = np.mean(full_pred_int[X_id] == full_y_int[X_id])
            failures = np.sum(full_pred_int[X_id] != full_y_int[X_id])

            acc = np.mean(full_pred_int == full_y_int)
            rmse_score = np.abs(acc_hat - acc)

            # Type I retraining
            concatenatedX, concatenatedy = fullX[X_id], fully[X_id]
            model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=train_hparam.learning_rate,
                                           clipvalue=train_hparam.clipvalue), metrics=[tf.keras.metrics.BinaryAccuracy()])
            model.fit(concatenatedX, concatenatedy, epochs=1, batch_size=128, verbose=0)

            mask = np.ones(fullX.shape[0], dtype=bool)
            mask[X_id] = False
            retrain_pred = model.predict(fullX[mask], verbose=0)
            softmax = np.hstack([1-retrain_pred, retrain_pred])
            retrain_pred_int = np.argmax(softmax, axis=1)  # (n_samples,)
            retrain_acc = np.mean(retrain_pred_int == full_y_int[mask])
            acc_clean = np.mean(full_pred_int[mask] == full_y_int[mask])
            acc_improvement = retrain_acc - acc_clean
            re = {
                'model': model_name,
                'test_set': test_set,
                'selection_metric': m,
                'budget': b,
                'acc': acc,
                'acc_hat': acc_hat,
                'failures': failures,
                'rmse': rmse_score,
                'retrain_acc': retrain_acc,
                'acc_improvement': acc_improvement
            }
            results.append(re)
            print(re)
    return results

def evaluate():
    vals = []
    originalX, originaly = trainX, trainy
    metricList = ['dr', 'ces', 'mcp', 'est', 'rnd', 'ent', 'gini', 'dat', 'gd', 'kmnc', 'nac', 'lsa', 'dsa', 'std', 'pace']

    for m in model_names:
        vals.extend(run_evaluation(m, 'andro', metricList, budgets, testX, testy, originalX, originaly))

    _X, _y = andro.get_ood('2018')
    vals.extend(run_evaluation('deepdrebin', 'andro_2018', metricList, budgets, _X, _y, originalX, originaly))

    _X, _y = andro.get_ood('2019')
    vals.extend(run_evaluation('deepdrebin', 'andro_2019', metricList, budgets, _X, _y, originalX, originaly))

    _X, _y = andro.get_ood('adv')
    vals.extend(run_evaluation('deepdrebin', 'andro_adv', metricList, budgets, _X, _y, originalX, originaly))

    _X, _y = andro.get_ood('label')
    vals.extend(run_evaluation('deepdrebin', 'andro_label', metricList, budgets, _X, _y, originalX, originaly))

    _X, _y = andro.get_ood('drebin')
    vals.extend(run_evaluation('deepdrebin', 'andro_drebin', metricList, budgets, _X, _y, originalX, originaly))

def evaluation_cluster(model_name, test_set, metricList, budgets):
    results = []
    for m in metricList:
        for b in budgets:
            test_out_dir = os.path.join(test_dir, test_set, model_name, m, str(b))
            if not os.path.exists(test_out_dir):
                raise FileNotFoundError(f"Test output directory {test_out_dir} does not exist.")
            mispred_indices = np.load(f'./test/andro/cluster_idx/{test_set}_{model_name}_{m}_{b}.npy')
            if len(mispred_indices) <= 2:
                clustering_results = {"Number of Clusters": 0,
                                      "Silhouette Score": -1.0,
                                      "DBCV Score": -1,
                                      "Combined Score": -1.0,
                                      "Number of Mispredicted Inputs": len(mispred_indices),
                                      "Number of Noisy Inputs": -1,
                                      "test_set": test_set,
                                      "selection_metric": m,
                                      "budget": b,
                                      "model": model_name,
                                      }
            else:
                suite_feat = np.load(f'./test/andro/cluster_feat/{test_set}_{model_name}_{m}_{b}.npy')
                u = cluster.umap_gpu(ip_mat=suite_feat, min_dist=0.1, n_components=min(25, len(suite_feat) - 1), n_neighbors=15,
                             metric='Euclidean')
                if np.isnan(u).any():
                    clustering_results = {"Number of Clusters": 0,
                                          "Silhouette Score": -1.0,
                                          "DBCV Score": -1,
                                          "Combined Score": -1.0,
                                          "Number of Mispredicted Inputs": len(mispred_indices),
                                          "Number of Noisy Inputs": -1,
                                          "test_set": test_set,
                                          "selection_metric": m,
                                          "budget": b,
                                          "model": model_name,
                                          }
                else:
                    optimal_eps = cluster.find_optimal_eps(u)
                    dbscan_float = DBSCAN(eps=optimal_eps,
                                          min_samples=2)
                    labels = dbscan_float.fit_predict(u)

                    if len(np.unique(labels)) == 1:
                        clustering_results = {
                            "Number of Clusters": labels.max() + 1,
                            "Silhouette Score": -1,
                            "DBCV Score": -1,
                            "Combined Score": -1,  # 0.5 * silhouette_umap + 0.5 * DBCV_score,
                            "Number of Mispredicted Inputs": len(u),
                            "Number of Noisy Inputs": list(labels).count(-1),
                            "test_set": test_set,
                            "selection_metric": m,
                            "budget": b,
                            "model": model_name
                        }
                    else:
                        silhouette_umap = sklearn.metrics.silhouette_score(u, labels)
                        DBCV_score = DBCV(u, labels)
                        clustering_results = {
                            "Number of Clusters": labels.max() + 1,
                            "Silhouette Score": silhouette_umap,
                            "DBCV Score": DBCV_score,
                            "Combined Score": 0.5 * silhouette_umap + 0.5 * DBCV_score,
                            "Number of Mispredicted Inputs": len(u),
                            "Number of Noisy Inputs": list(labels).count(-1),
                            "test_set": test_set,
                            "selection_metric": m,
                            "budget": b,
                            "model": model_name
                        }
            print(clustering_results)
            results.append(clustering_results)
    return results

def evaluate_cluster():
    vals = []
    metricList = ['dr', 'ces', 'mcp', 'est', 'rnd', 'ent', 'gini', 'dat', 'gd', 'kmnc', 'nac', 'lsa', 'dsa', 'std', 'pace']
    for m in model_names:
        vals.extend(evaluation_cluster(m, 'andro', metricList, budgets))
        save_results(f'{m}_andro_cluster', vals)

    for v in ['2018', '2019', 'adv', 'label', 'drebin']:
        _X, _y = andro.get_ood(v)
        vals.extend(evaluation_cluster('deepdrebin', 'andro_'+v, metricList, budgets))
        save_results(f'deepdrebin_{v}_cluster', vals)

def save_idx(model_name, fullX, fully, test_set):
    metricList = ['dr', 'ces', 'mcp', 'est', 'rnd', 'ent', 'gini', 'dat', 'gd', 'kmnc', 'nac', 'lsa', 'dsa', 'std', 'pace']
    model = get_model(model_name)
    full_y_int = fully
    full_pred = model.predict(fullX, verbose=0)

    p1 = full_pred
    p0 = 1 - p1
    # Concatenate to get (10000, 2) where each row is [P(class=0), P(class=1)]
    softmax = np.hstack([p0, p1])
    full_pred_int = np.argmax(softmax, axis=1)  # (n_samples,)

    for m in metricList:
        for b in budgets:
            test_out_dir = os.path.join(test_dir, test_set, model_name, m, str(b))
            X_id = np.loadtxt(os.path.join(test_out_dir, 'X.txt'), dtype=int)  # (n_selected,)
            idx = full_pred_int[X_id] != full_y_int[X_id]
            mispred_indices = X_id[idx]
            root = f'./test/andro/cluster_idx/'
            if not os.path.exists(root):
                os.makedirs(root)
            np.save(root+f'{test_set}_{model_name}_{m}_{b}.npy', mispred_indices)

def save_feat(test_set, model_name, fullX, force_save=False):
    metricList = ['dr', 'ces', 'mcp', 'est', 'rnd', 'ent', 'gini', 'dat', 'gd', 'kmnc', 'nac', 'lsa', 'dsa', 'std', 'pace']
    model = get_model('deepdrebin')
    dense1 = model.layers[0].output
    feat_layer = Model(inputs=model.input, outputs=dense1)
    for m in metricList:
        for b in budgets:
            path = f'./test/andro/cluster_feat/{test_set}_{model_name}_{m}_{b}.npy'
            if os.path.exists(path) and force_save==False:
                continue
            mispred_indices = np.load(f'./test/andro/cluster_idx/{test_set}_{model_name}_{m}_{b}.npy')
            apks = fullX[mispred_indices]
            if len(apks) == 0:
                print('empty apk', f'{test_set}_{model_name}_{m}_{b}')
                np.save(path, np.array([]))
            else:
                feat = feat_layer.predict(apks)
                np.save(path, feat)

def run_save():
    for model_name in model_names:
        save_idx(model_name, testX, testy, 'andro')
    for v in ['2018', '2019', 'adv', 'drebin', 'label']:
        _X, _y = andro.get_ood(v)
        save_idx('deepdrebin', _X, _y, 'andro_'+v)

def run_save_feat():
    for model_name in model_names:
        save_feat('andro', model_name, testX, True)
    for v in ['2018', '2019', 'adv', 'drebin', 'label']:
        _X, _y = andro.get_ood(v)
        save_feat('andro_'+v, 'deepdrebin', _X, True)


def time_cost(model_name, test_set, testX, testy, metricList, csv='report/andro_time.csv'):
    b = 200
    model = get_model(model_name)
    for m in metricList:
        if m == 'dat':
            n_test = testX.shape[0]
            hybridX = np.concatenate((trainX[:int(n_test // 2)], testX[:int(n_test // 2)]), axis=0)
            hybridy = np.concatenate((trainy[:int(n_test // 2)], testy[:int(n_test // 2)]), axis=0)
            start = time.time()
            selectedX, selectedy, idx = metrics.dat_ood_detector(
                testX, testy, model, b, test_set, trainX, trainy, hybridX, hybridy, batch_size=128,
                num_classes=10
            )
            time_cost = time.time() - start
        else:
            start = time.time()
            selectedX, selectedy, idx = metrics.select(trainX, trainy,
                                                       testX, testy, model, b, m, test_set, model_name
                                                       )
            time_cost = time.time() - start


def time_efficiency():
    metricList = ['lsa', 'dsa', 'est', 'dr', 'mcp', 'ces', 'rnd', 'ent', 'gini', 'dat', 'gd', 'kmnc', 'nac', 'std', 'pace']
    for m in model_names:
        time_cost(m, 'andro', testX, testy, metricList)

    for v in ['2018', '2019', 'adv', 'drebin', 'label']:
        _X, _y = andro.get_ood(v)
        time_cost('deepdrebin', 'andro_'+v, _X, _y, metricList)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run AndroZoo experiments")
    parser.add_argument(
        "--exp",
        type=str,
        required=True,
        help="Experiment to run: select, evaluate, evaluate_cluster, time_efficiency"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Model name (only needed for retrain_cluster)"
    )

    args = parser.parse_args()
    if args.exp == "select":
        select()
    elif args.exp == "evaluate":
        evaluate()
    elif args.exp == "evaluate_cluster":
        evaluate_cluster()
    elif args.exp == "time_efficiency":
        time_efficiency()
    else:
        raise ValueError(f"Unknown experiment: {args.exp}")